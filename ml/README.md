<div align="center">
<br />
  <img src="https://www.simplon.ma/images/Simplon_Maghreb_Rouge.png" alt="Simplon Maghreb Logo" width="300"/>
  <br />
  <br />
  <h1>ü§ñ Documentation Machine Learning</h1>
  <p><strong>Justification, Analyse Exploratoire & Performance du Mod√®le</strong></p>
  <br />

  <div>
    <img src="https://img.shields.io/badge/-Scikit_Learn-black?style=for-the-badge&logo=scikit-learn&logoColor=white&color=F7931E" />
    <img src="https://img.shields.io/badge/-Pandas-black?style=for-the-badge&logo=pandas&logoColor=white&color=150458" />
    <img src="https://img.shields.io/badge/-SMOTE-black?style=for-the-badge&logo=python&logoColor=white&color=3776AB" />
  </div>
</div>

---

## üìä 1. Analyse Exploratoire (EDA)

Avant la mod√©lisation, l'analyse des donn√©es a permis d'identifier les facteurs critiques influen√ßant le d√©part des employ√©s ("Attrition").

### Facteurs Cl√©s Identifi√©s
L'analyse visuelle (Boxplots) r√©v√®le des diff√©rences significatives entre les employ√©s qui partent et ceux qui restent :

![Facteurs Cl√©s EDA](./documentation/images/eda_factors.png)

1.  **Revenu Mensuel (`MonthlyIncome`)** : Les employ√©s qui partent ont tendance √† avoir un salaire m√©dian nettement inf√©rieur.
2.  **√Çge (`Age`)** : Les d√©parts sont plus fr√©quents chez les jeunes employ√©s (d√©but de carri√®re).
3.  **Anciennet√© (`TotalWorkingYears`)** : Le turnover est plus fort chez les profils ayant moins d'exp√©rience globale.

---

## üß† 2. S√©lection et Justification du Mod√®le

Nous avons compar√© deux approches pour r√©soudre ce probl√®me de classification binaire : **R√©gression Logistique** (mod√®le lin√©aire) et **Random Forest** (mod√®le ensembliste non-lin√©aire).

### Comparaison des Performances (AUC)

Le m√©trique principal retenu est l'**AUC (Area Under Curve)** car il √©value la capacit√© du mod√®le √† distinguer les classes ind√©pendamment du seuil de d√©cision.

![Comparaison Mod√®les](./documentation/images/model_comparison.png)

| Mod√®le | Score AUC | Observation |
|:-------|:----------|:------------|
| R√©gression Logistique | 0.7808 | Performant mais limit√© par la lin√©arit√©. |
| **Random Forest** | **0.7940** | **Meilleure performance globale.** |

### Pourquoi Random Forest ?
Le choix s'est port√© sur le **Random Forest** pour trois raisons majeures :
1.  **Non-lin√©arit√©** : Les donn√©es RH contiennent des relations complexes (ex: un salaire bas n'est pas g√™nant si l'√¢ge est faible, mais critique si l'√¢ge est √©lev√©). Le Random Forest capture ces interactions mieux qu'une r√©gression lin√©aire.
2.  **Robustesse (Bagging)** : En moyennant plusieurs arbres de d√©cision, il r√©duit le risque de sur-apprentissage (overfitting), un point critique sur ce dataset de petite taille (~1400 lignes).
3.  **Gestion du D√©s√©quilibre** : Coupl√© √† **SMOTE**, le Random Forest parvient mieux √† isoler la classe minoritaire (les d√©parts).

---

## ‚öôÔ∏è 3. Optimisation et Pipeline

Pour maximiser la performance, le pipeline suivant a √©t√© mis en place :

1.  **Pr√©traitement** : Encodage (`LabelEncoder`) et Normalisation.
2.  **√âquilibrage** : Utilisation de **SMOTE** pour g√©n√©rer des donn√©es synth√©tiques sur la classe minoritaire ("Yes").
3.  **GridSearch** : Optimisation des hyperparam√®tres.
    * *Meilleurs param√®tres trouv√©s* : `max_depth=5`, `min_samples_leaf=2`.

---

## üìà 4. R√©sultats Finaux

### Rapport de Classification
Le mod√®le final pr√©sente une **Accuracy de 84%**.

| Classe | Pr√©cision | Rappel (Recall) | F1-Score | Support |
|:-------|:----------|:----------------|:---------|:--------|
| **0 (Reste)** | 0.91 | 0.90 | 0.90 | 247 |
| **1 (D√©part)**| **0.50** | **0.53** | **0.52** | **47** |
| *Moyenne* | *0.84* | *0.84* | *0.84* | *294* |

### Matrice de Confusion
L'analyse de la matrice de confusion montre la capacit√© du mod√®le √† d√©tecter les d√©parts r√©els :

![Matrice de Confusion](./documentation/images/confusion_matrix.png)

* **Vrais Positifs (TP) : ~25** employ√©s d√©tect√©s correctement comme "√† risque".
* **Faux N√©gatifs (FN) : ~22** employ√©s dont le d√©part n'a pas √©t√© anticip√©.

> **Note :** Bien que la pr√©cision sur la classe 1 soit de 50%, le **Rappel (Recall) de 53%** est un r√©sultat encourageant pour ce type de probl√®me d√©s√©quilibr√©. Cela signifie que nous parvenons √† identifier plus de la moiti√© des d√©missionnaires potentiels, permettant aux RH d'agir pr√©ventivement.